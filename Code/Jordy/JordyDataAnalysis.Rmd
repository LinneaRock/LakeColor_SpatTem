---
title: "Jordy_Data_Analysis"
author: "Jordan Von Eggers"
date: "2023-10-18"
output: html_document
editor_options: 
  chunk_output_type: console
---

# 0. Set up on Beartooth 
Request resources, move to directory, and load modules for R
```{bash}
ssh jvonegge@beartooth.arcc.uwyo.edu
cd /project/lakecolor/data/

salloc --mem=110GB --nodes=1 --cpus-per-task=1 --account=microbiome --time=2:00:00

module load arcc/1.0  gcc/12.2.0 
module load r/4.2.2
R
```

```{r}
require(tidyverse)
#color for manually created legend
bg.fui = tibble(
  ymin = c(470,475,480,485,489,495,509,530,549,559,564,567,568,569,570,573,575,577,579,581,583),
  ymax = c(475,480,485,489,495,509,530,549,559,564,567,568,569,570,573,575,577,579,581,583,590),
  color = c(
  "#2158bc", "#316dc5", "#327cbb", "#4b80a0", "#568f96", "#6d9298", "#698c86", 
  "#759e72", "#7ba654", "#7dae38", "#94b660","#94b660", "#a5bc76", "#aab86d", 
  "#adb55f", "#a8a965", "#ae9f5c", "#b3a053", "#af8a44", "#a46905", "#9f4d04")
)
```



# 1. Histogram of temporal lake size
```{r}
load("2023-10-19_TEMPORAL_Limnosat_May_Oct_Lagos_Filtered.RData")


pdf(paste0(Sys.Date(),"_TEMPORAL_HistogramLakeSize.pdf"), height=8,width=7)
hist(temporal$lake_waterarea_ha, xlim=c(0,1000),breaks=100000)
dev.off()


uniq_lakes <-temporal[,names(temporal)%in%c("lake_nhdid","lake_waterarea_ha")]
uniq_lakes<-unique(uniq_lakes)

uniq_lakes_small<-uniq_lakes[uniq_lakes$lake_waterarea_ha<11,]


pdf(paste0("/gscratch/jvonegge/LakeColor/",Sys.Date(),"_TEMPORAL_HistogramLakeSize_SmallLakes.pdf"), height=8,width=7)
hist(uniq_lakes_small$lake_waterarea_ha,ylab="Freq", xlab="Lake size (hectares)", main="Histogram of temporal dataset lakes < 11 ha")
dev.off()


uniq_lakes_med<-uniq_lakes[uniq_lakes$lake_waterarea_ha<100,]

pdf(paste0("/gscratch/jvonegge/LakeColor/",Sys.Date(),"_TEMPORAL_HistogramLakeSize_LakesLessThan100ha.pdf"), height=8,width=7)
hist(uniq_lakes_med$lake_waterarea_ha, ylab="Freq", xlab="Lake size (hectares)", main="Histogram of temporal dataset lakes < 100 ha")
dev.off()

rm(list=setdiff(ls(), "temporal"))

```

# 2. run linear models for temporal dataset
```{r}
load("../data/2023-10-19_TEMPORAL_Limnosat_May_Oct_Lagos_Filtered.RData")

sub_temporal_yravgdWL<-data.frame()
temporal_yravgdWL<-data.frame()

lm_model_results<-data.frame(lake_nhdid=character(), lm_yint=numeric(), lm_pval=numeric(), lm_rsq=numeric(),lm_slope=numeric(), resid_yint=numeric(), resid_pval=numeric(), resid_rsq=numeric(),resid_slope=numeric())


i=1
j=1
for(i in 1:length(unique(temporal$lake_nhdid))){
        sub<-temporal[temporal$lake_nhdid==unique(temporal$lake_nhdid)[i],]
        for(j in 1:length(unique(sub$year))){
                sub2<-sub[sub$year==unique(sub$year)[j],]
                sub2[1,]$dWL<-mean(sub2$dWL)
                sub_temporal_yravgdWL<-rbind(sub_temporal_yravgdWL,sub2[1,])
        }
        
        temp_lm<-lm(sub_temporal_yravgdWL$dWL~sub_temporal_yravgdWL$year)
        sub_temporal_yravgdWL$residuals<-abs(temp_lm$residuals)
        resid_lm<-lm(sub_temporal_yravgdWL$residuals~sub_temporal_yravgdWL$year)
        
        lm_model_results<- rbind(lm_model_results,data.frame(lake_nhdid=unique(temporal$lake_nhdid)[i], lm_yint=summary(temp_lm)$coefficients[1,1], lm_pval=summary(temp_lm)$coefficients[2,4], lm_rsq=summary(temp_lm)$r.squared, lm_slope=summary(temp_lm)$coefficients[2,1], resid_yint=summary(resid_lm)$coefficients[1,1], resid_pval=summary(resid_lm)$coefficients[2,4], resid_rsq=summary(resid_lm)$r.squared,resid_slope=summary(resid_lm)$coefficients[2,1]))
        
        temporal_yravgdWL<-rbind(temporal_yravgdWL,sub_temporal_yravgdWL)
        rm(sub_temporal_yravgdWL)
        sub_temporal_yravgdWL<-data.frame()
        print(i)
}

rm(list=setdiff(ls(), "lm_model_results","temporal_yravgdWL"))
save.image(paste0(Sys.Date(),"TEMPORAL_LinearModelOutput.Rdata"))
```


run_2023-10-30_TEMPORAL_LinearModel.sh
```{bash}
#!/bin/bash
#SBATCH --job-name temporal_lms
#SBATCH --mem=100GB
#SBATCH --time=36:00:00
#SBATCH --cpus-per-task=1
#SBATCH --account=microbiome
#SBATCH --output=temporal_lms_%A.out

cd /project/lakecolor/data_analysis
module load arcc/1.0  gcc/12.2.0 
module load r/4.2.2

srun Rscript 2023-10-30_TEMPORAL_LinearModel_1.R
date
```


```{bash}



[jvonegge@blog2 data_analysis]$ sbatch run_2023-10-30_TEMPORAL_LinearModel_1.sh 
Submitted batch job 11254813
[jvonegge@blog2 data_analysis]$ sbatch run_2023-10-30_TEMPORAL_LinearModel_2.sh 
Submitted batch job 11254817
[jvonegge@blog2 data_analysis]$ sbatch run_2023-10-30_TEMPORAL_LinearModel_3.sh 
Submitted batch job 11254820
[jvonegge@blog2 data_analysis]$ sbatch run_2023-10-30_TEMPORAL_LinearModel_4.sh 
Submitted batch job 11254824
[jvonegge@blog2 data_analysis]$ sbatch run_2023-10-30_TEMPORAL_LinearModel_5.sh 
Submitted batch job 11254827
[jvonegge@blog2 data_analysis]$ sbatch run_2023-10-30_TEMPORAL_LinearModel_6.sh 
Submitted batch job 11254830
[jvonegge@blog2 data_analysis]$ sbatch run_2023-10-30_TEMPORAL_LinearModel_7.sh 
Submitted batch job 11254834
[jvonegge@blog2 data_analysis]$ sbatch run_2023-10-30_TEMPORAL_LinearModel_8.sh 

tail temporal_lms_11254813.out
tail temporal_lms_11254817.out
tail temporal_lms_11254820.out
tail temporal_lms_11254824.out
tail temporal_lms_11254827.out
tail temporal_lms_11254830.out
tail temporal_lms_11254834.out
tail temporal_lms_11254837.out

```

bind all temporal datasets together
```{r}
compiled_lm_model_results<-NULL
compiled_temporal_yravgdWL<-NULL
load("2023-11-08TEMPORAL_LinearModelOutput_1.Rdata")
compiled_lm_model_results<-lm_model_results
compiled_temporal_yravgdWL<-temporal_yravgdWL
load("2023-11-08TEMPORAL_LinearModelOutput_2.Rdata")
compiled_lm_model_results<-rbind(compiled_lm_model_results, lm_model_results)
compiled_temporal_yravgdWL<-rbind(compiled_temporal_yravgdWL,temporal_yravgdWL)
load("2023-11-08TEMPORAL_LinearModelOutput_3.Rdata")
compiled_lm_model_results<-rbind(compiled_lm_model_results, lm_model_results)
compiled_temporal_yravgdWL<-rbind(compiled_temporal_yravgdWL,temporal_yravgdWL)
load("2023-11-08TEMPORAL_LinearModelOutput_4.Rdata")
compiled_lm_model_results<-rbind(compiled_lm_model_results, lm_model_results)
compiled_temporal_yravgdWL<-rbind(compiled_temporal_yravgdWL,temporal_yravgdWL)
load("2023-11-08TEMPORAL_LinearModelOutput_5.Rdata")
compiled_lm_model_results<-rbind(compiled_lm_model_results, lm_model_results)
compiled_temporal_yravgdWL<-rbind(compiled_temporal_yravgdWL,temporal_yravgdWL)
load("2023-11-08TEMPORAL_LinearModelOutput_6.Rdata")
compiled_lm_model_results<-rbind(compiled_lm_model_results, lm_model_results)
compiled_temporal_yravgdWL<-rbind(compiled_temporal_yravgdWL,temporal_yravgdWL)
load("2023-11-08TEMPORAL_LinearModelOutput_7.Rdata")
compiled_lm_model_results<-rbind(compiled_lm_model_results, lm_model_results)
compiled_temporal_yravgdWL<-rbind(compiled_temporal_yravgdWL,temporal_yravgdWL)
load("2023-11-09TEMPORAL_LinearModelOutput_8.Rdata")
compiled_lm_model_results<-rbind(compiled_lm_model_results, lm_model_results)
compiled_temporal_yravgdWL<-rbind(compiled_temporal_yravgdWL,temporal_yravgdWL)

length(unique(compiled_lm_model_results$lake_nhdid))
#[1] 81035
length(unique(compiled_temporal_yravgdWL$lake_nhdid))
#[1] 81035

#here I need to remove brightness, normalized_green and Nir because they were NOT averaged to make the datafame
compiled_temporal_yravgdWL$brightness<-NULL
compiled_temporal_yravgdWL$normalized_green<-NULL
compiled_temporal_yravgdWL$Nir<-NULL

#rename the rownames for the compiled dataset
rownames(compiled_temporal_yravgdWL)<-seq(1,nrow(compiled_temporal_yravgdWL),1) 

write.csv(compiled_temporal_yravgdWL,paste0("../",Sys.Date(),"_TEMPORAL_yearly_averaged_dWL.csv"))
write.csv(compiled_lm_model_results,paste0(Sys.Date(),"_TEMPORAL_linear_model_results.csv"))

```

add in LAGOS data

```{bash}
cd /project/lakecolor/data_analysis/TEMPORAL_LinearModels
R
```

```{r}
require(tidyverse)
lm_res<-read.csv("2023-11-09_TEMPORAL_linear_model_results.csv", header=T, row.names=1)
nrow(lm_res)
lake_data<-readRDS("/project/lakecolor/data/archive/2023-09-29_lagos_landcover_data.rds") #this is the lake data and landcover - need to remove land cover 

# merge with Lagos data (should be static for each lake)
# first take out the landcover data that was attached
lagos<-lake_data[,1:15]
lagos<-distinct(lagos)
nrow(lagos)
#479950

table(unique(lm_res$lake_nhdid)%in%unique(lagos$lake_nhdid))
#  TRUE 
# 81035
 
#merge the lm_res dataset with the filtered lagos dataset
lm_res_lagos<-inner_join(lm_res,lagos)
#Joining with `by = join_by(lake_nhdid)`

nrow(lm_res_lagos)
#[1] 81035

write.csv(lm_res_lagos, paste0(Sys.Date(), "_TEMPORAL_linear_model_results_lagos.csv"))

```

## a. explore lm results
copy to local drive
```{r}
lm_res<-read.csv("/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/Collaborations/LakeColor/2023-11-09_TEMPORAL_linear_model_results_lagos.csv", header = T, row.names=1)

require(ggplot2)

# Get state borders
states <- map_data("state")

cols <- c(
  "dodgerblue2", "#E31A1C", # red
  "green4",
  "#6A3D9A", # purple
  "#FF7F00", # orange
   "gold1",
  "skyblue2", "#FB9A99", # lt pink
  "palegreen2",
  "#FDBF6F", # lt orange
  "maroon", "orchid1","black", "deeppink1", "blue1", "steelblue4",
  "darkturquoise", "green1", "yellow4", "yellow3",
  "darkorange4", "brown","gray","khaki2","#CAB2D6"
)

lm_res$ecoregion_color <- cols[as.factor(lm_res$epanutr_zoneid )]

wmap<-ggplot(data = lm_res) + 
        geom_point(data = lm_res, aes(x = lake_lon_decdeg, y = lake_lat_decdeg), color = lm_res$ecoregion_color, size = 1) +  # Add points
        scale_fill_distiller( palette="Spectral", direction=1)+
        theme_void()+
       coord_fixed(1.3) +
        geom_polygon(data= states, aes(x = long, y = lat, group = group), fill="transparent",color = "black", linewidth=0.5)
pdf(paste0("../",Sys.Date(),"_Temporal_EcoRegionsMap.pdf"),height=4,width=6)
wmap
dev.off()
```

```{r}
lm_res<-read.csv("/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/Collaborations/LakeColor/2023-11-09_TEMPORAL_linear_model_results_lagos.csv", header = T, row.names=1)

cols <- c(
  "dodgerblue2", "#E31A1C", # red
  "green4",
  "#6A3D9A", # purple
  "#FF7F00", # orange
   "gold1",
  "skyblue2", "#FB9A99", # lt pink
  "palegreen2",
  "#FDBF6F", # lt orange
  "maroon", "orchid1","black", "deeppink1", "blue1", "steelblue4",
  "darkturquoise", "green1", "yellow4", "yellow3",
  "darkorange4", "brown","gray","khaki2","#CAB2D6"
)

lm_res$ecoregion_color <- cols[as.factor(lm_res$epanutr_zoneid )]


lm_res$lake_size_cat<-rep(NA, nrow(lm_res))
lm_res[lm_res$lake_waterarea_ha>=10,]$lake_size_cat<-"large"
lm_res[lm_res$lake_waterarea_ha<10,]$lake_size_cat<-"small"
large<-lm_res[lm_res$lake_size_cat=="large",]
small<-lm_res[lm_res$lake_size_cat=="small",]



#two lines for size
x=seq(1984,2021,1)
pdf(paste0("../",Sys.Date(),"_TemporalTrends_SmallLarge.pdf"),height=4,width=6)
par(mar=c(4,4,2,6))
        plot(x,y=seq(400,600,length.out=length(x)), col="white", ylab="Forel-Ule color (dwl)", xlab="Years (Common Era)", ylim=c(570,500))

i=1
for(i in 1:length(unique(large$epanutr_zoneid))){
        sub<-large[large$epanutr_zoneid==unique(large$epanutr_zoneid)[i],]
        y=mean(sub$lm_yint)+(mean(sub$lm_slope)*x)
        lines(x,y,type="l", col=unique(sub$ecoregion_color), lwd=5 )
}

i=1
for(i in 1:length(unique(small$epanutr_zoneid))){
        sub<-small[small$epanutr_zoneid==unique(small$epanutr_zoneid)[i],]
        y=mean(sub$lm_yint)+(mean(sub$lm_slope)*x)
        lines(x,y,type="l", col=unique(sub$ecoregion_color), lty=2,lwd=5 )
}
dev.off()
```

#assign color values

```{bash}
rsync jvonegge@beartooth.arcc.uwyo.edu:/project/lakecolor/data_analysis/2023-11-09_TEMPORAL_yearly_averaged_dWL.csv /Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/Collaborations/LakeColor
```

1. round values in dataset
2. merge values
```{r}
YrAvg<-read.csv("/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/Collaborations/LakeColor/2023-11-09_TEMPORAL_yearly_averaged_dWL.csv", header=T)

require(tidyverse)
#color for manually created legend
bg.fui = tibble(
  ymin = c(470,475,480,485,489,495,509,530,549,559,564,567,568,569,570,573,575,577,579,581,583),
  ymax = c(475,480,485,489,495,509,530,549,559,564,567,568,569,570,573,575,577,579,581,583,590),
  color = c(
  "#2158bc", "#316dc5", "#327cbb", "#4b80a0", "#568f96", "#6d9298", "#698c86", 
  "#759e72", "#7ba654", "#7dae38", "#94b660","#94b660", "#a5bc76", "#aab86d", 
  "#adb55f", "#a8a965", "#ae9f5c", "#b3a053", "#af8a44", "#a46905", "#9f4d04")
)


site_cols <- YrAvg %>%
  mutate(dWL = round(dWL,0)) 

bg.fui<-as.data.frame(bg.fui)

YrAvg <- YrAvg %>% mutate(col_hex = case_when(dWL <= bg.fui[1,1] & dWL <bg.fui[1,2] ~bg.fui[1,3],          dWL <=bg.fui[2,1] & dWL <bg.fui[2,2] ~bg.fui[2,3], 
         dWL <=bg.fui[3,1] & dWL <bg.fui[3,2] ~bg.fui[3,3]   , 
         dWL <=bg.fui[4,1] & dWL <bg.fui[4,2] ~bg.fui[4,3]   , 
         dWL <=bg.fui[5,1] & dWL <bg.fui[5,2] ~bg.fui[5,3]   , 
         dWL <=bg.fui[6,1] & dWL <bg.fui[6,2] ~bg.fui[6,3]   , 
         dWL <=bg.fui[7,1] & dWL <bg.fui[7,2] ~bg.fui[7,3]   , 
         dWL <=bg.fui[8,1] & dWL <bg.fui[8,2] ~bg.fui[8,3]   , 
         dWL <=bg.fui[9,1] & dWL <bg.fui[9,2] ~bg.fui[9,3]   , 
         dWL <=bg.fui[10,1] & dWL <bg.fui[10,2] ~bg.fui[10,3]   , 
         dWL <=bg.fui[11,1] & dWL <bg.fui[11,2] ~bg.fui[11,3]   , 
         dWL <=bg.fui[12,1] & dWL <bg.fui[12,2] ~bg.fui[12,3]   , 
         dWL <=bg.fui[13,1] & dWL <bg.fui[13,2] ~bg.fui[13,3]   , 
         dWL <=bg.fui[14,1] & dWL <bg.fui[14,2] ~bg.fui[14,3]   , 
         dWL <=bg.fui[15,1] & dWL <bg.fui[15,2] ~bg.fui[15,3]   , 
         dWL <=bg.fui[16,1] & dWL <bg.fui[16,2] ~bg.fui[16,3]   , 
         dWL <=bg.fui[17,1] & dWL <bg.fui[17,2] ~bg.fui[17,3]   , 
         dWL <=bg.fui[18,1] & dWL <bg.fui[18,2] ~bg.fui[18,3]   , 
         dWL <=bg.fui[19,1] & dWL <bg.fui[19,2] ~bg.fui[19,3]   , 
         dWL <=bg.fui[20,1] & dWL <bg.fui[20,2] ~bg.fui[20,3]   , 
         dWL <=bg.fui[21,1] & dWL <bg.fui[21,2] ~bg.fui[21,3] , T~"black")) 
  
        
if        
        
        
        
        
        
        
        mutate(col_hex = case_when(dWL <=475 & dWL <480 )~"#316dc5"  %>% 
        mutate(col_hex = case_when(dWL <=480 & dWL <485 )~"#327cbb"  %>% 
        mutate(col_hex = case_when(dWL <=485 & dWL <489 )~"#4b80a0"  %>% 
        mutate(col_hex = case_when(dWL <=489 & dWL <495 )~"#568f96"  %>% 
        mutate(col_hex = case_when(dWL <=509 & dWL <530 )~"#698c86"  %>% 
        mutate(col_hex = case_when(dWL <=495 & dWL <509 )~"#6d9298"  %>% 
        530
549
#759e72
        mutate(col_hex = case_when(dWL <= & dWL <475 )~"#2158bc"  %>% 
        mutate(col_hex = case_when(dWL <=470 & dWL <475 )~"#2158bc"  %>% 
        mutate(col_hex = case_when(dWL <=470 & dWL <475 )~"#2158bc"  %>% 
        mutate(col_hex = case_when(dWL <=470 & dWL <475 )~"#2158bc"  %>% 
    

```



























# make map of western states
```{r}
lm_res<-read.csv("/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/Collaborations/LakeColor/2023-11-09_TEMPORAL_linear_model_results_lagos.csv", header = T, row.names=1)
states <- map_data("state")
west <- subset(states, region %in% c("washington", "oregon", "california", "nevada", "idaho", "montana", "wyoming", "colorado", "arizona", "new mexico"))


cols<-c(cols<-c("#43b284","#fab255","darkgray", "#0f7ba2" ,  "#dd5129"  ))

lm_res<-lm_res[lm_res$lake_lon_decdeg<= (-102.05),]
lm_res$ecoregion_color <- cols[as.factor(lm_res$epanutr_zoneid )]
lm_res$lake_size_cat<-rep(NA, nrow(lm_res))
lm_res[lm_res$lake_waterarea_ha>=10,]$lake_size_cat<-"large"
lm_res[lm_res$lake_waterarea_ha<10,]$lake_size_cat<-"small"

unique(lm_res[,names(lm_res)%in%c("epanutr_zoneid","ecoregion_color")])


      epanutr_zoneid ecoregion_color
29         epanutr_9        "#dd5129"
282        epanutr_8        "#0f7ba2"
545        epanutr_3        "#43b284"
5637       epanutr_5         "#fab255"
29547      epanutr_6        "darkgray"
wmap<-ggplot(data = lm_res) + 
        geom_point(data = lm_res, aes(x = lake_lon_decdeg, y = lake_lat_decdeg), color = lm_res$ecoregion_color, size = 1) +  # Add points
        scale_fill_distiller( palette="Spectral", direction=1)+
        theme_void()+
       coord_fixed(1.3) +
        geom_polygon(data= west, aes(x = long, y = lat, group = group), fill="transparent",color = "black", size=0.5)
wmap

west<-lm_res[lm_res$epanutr_zoneid!="epanutr_6",]
large<-west[west$lake_size_cat=="large",]
small<-west[west$lake_size_cat=="small",]

x=seq(1984,2021,1)

pdf("../LakeColorWest.pdf",height=4,width=5)
par(mar=c(4,4,2,1))
        plot(x,y=seq(400,600,length.out=length(x)), col="white", ylab="Forel-Ule color (dwl)", xlab="Years (Common Era)", ylim=c(515,560))

i=1
for(i in 1:length(unique(west$epanutr_zoneid))){
        sub<-west[west$epanutr_zoneid==unique(west$epanutr_zoneid)[i],]
y=mean(sub$lm_yint)+(mean(sub$lm_slope)*x)
lines(x,y,type="l", col=unique(sub$ecoregion_color), lwd=5 )
}
dev.off()


#two lines for size
x=seq(1984,2021,1)
pdf("../LakeColorWest_size.pdf",height=4,width=5)
par(mar=c(4,4,2,1))
        plot(x,y=seq(400,600,length.out=length(x)), col="white", ylab="Forel-Ule color (dwl)", xlab="Years (Common Era)", ylim=c(510,560))

i=1
for(i in 1:length(unique(large$epanutr_zoneid))){
        sub<-large[large$epanutr_zoneid==unique(large$epanutr_zoneid)[i],]
        y=mean(sub$lm_yint)+(mean(sub$lm_slope)*x)
        lines(x,y,type="l", col=unique(sub$ecoregion_color), lwd=5 )
}

i=1
for(i in 1:length(unique(small$epanutr_zoneid))){
        sub<-small[small$epanutr_zoneid==unique(small$epanutr_zoneid)[i],]
        y=mean(sub$lm_yint)+(mean(sub$lm_slope)*x)
        lines(x,y,type="l", col=unique(sub$ecoregion_color), lty=2,lwd=5 )
}
dev.off()





#two lines for size
x=seq(1984,2021,1)
pdf("../LakeColorWest_size_rev.pdf",height=4,width=5)
par(mar=c(4,4,2,1))
        plot(x,y=seq(400,600,length.out=length(x)), col="white", ylab="Forel-Ule color (dwl)", xlab="Years (Common Era)", ylim=c(560,510), xlim=c(2021,1984))

i=1
for(i in 1:length(unique(large$epanutr_zoneid))){
        sub<-large[large$epanutr_zoneid==unique(large$epanutr_zoneid)[i],]
        y=mean(sub$lm_yint)+(mean(sub$lm_slope)*x)
        lines(x,y,type="l", col=unique(sub$ecoregion_color), lwd=5 )
}

i=1
for(i in 1:length(unique(small$epanutr_zoneid))){
        sub<-small[small$epanutr_zoneid==unique(small$epanutr_zoneid)[i],]
        y=mean(sub$lm_yint)+(mean(sub$lm_slope)*x)
        lines(x,y,type="l", col=unique(sub$ecoregion_color), lty=2,lwd=5 )
}
dev.off()
plot.new()
legend("center",legend=c("large","small"), lty=c(1,2), lwd=3, title="Lake size", bty="n")

```





# Figuring out why no TX and GA

```{r}
require(tidyverse)
require(ggplot2)
lake_data<-readRDS("2023-09-29_lagos_landcover_data.rds") #this is the lake data and landcover - need to remove land cover 

# merge with Lagos data (should be static for each lake)
# first take out the landcover data that was attached
lagos<-lake_data[,1:15]
lagos<-distinct(lagos)
nrow(lagos)
#479950

write.csv(lagos,"LAGOSONLYDATA_TEST.csv")


lagos<-read.csv("../LAGOSONLYDATA_TEST.csv")


require(ggplot2)

# Get state borders
states <- map_data("state")

cols <- c(
  "dodgerblue2", "#E31A1C", # red
  "green4",
  "#6A3D9A", # purple
  "#FF7F00", # orange
  "black", "gold1",
  "skyblue2", "#FB9A99", # lt pink
  "palegreen2",
  "#FDBF6F", # lt orange
  "maroon", "orchid1", "deeppink1", "blue1", "steelblue4",
  "darkturquoise", "green1", "yellow4", "yellow3",
  "darkorange4", "brown","gray","khaki2","#CAB2D6"
)

lagos$ecoregion_color <- cols[as.factor(lagos$epanutr_zoneid )]

wmap<-ggplot(data = lagos) + 
        geom_point(data = lagos, aes(x = lake_lon_decdeg, y = lake_lat_decdeg), color = lagos$ecoregion_color, size = 1) +  # Add points
        scale_fill_distiller( palette="Spectral", direction=1)+
        theme_void()+
       coord_fixed(1.3) +
        geom_polygon(data= states, aes(x = long, y = lat, group = group), fill="transparent",color = "black", size=0.5)
wmap





require(tidyverse)
require(ggplot2)
load("2023-10-19_SPATIAL_Limnosat_May_Oct_Lagos_Landcover.RData")

spatial<-spatial[,names(spatial)%in% c("epanutr_zoneid","lake_lon_decdeg","lake_lat_decdeg")]
spatial<-distinct(spatial)

# Get state borders
states <- map_data("state")

cols <- c(
  "dodgerblue2", "#E31A1C", # red
  "green4",
  "#6A3D9A", # purple
  "#FF7F00", # orange
   "gold1",
  "skyblue2", "#FB9A99", # lt pink
  "palegreen2",
  "#FDBF6F", # lt orange
  "maroon", "orchid1", "black","deeppink1", "blue1", "steelblue4",
  "darkturquoise", "green1", "yellow4", "yellow3",
  "darkorange4", "brown","gray","khaki2","#CAB2D6"
)

spatial$ecoregion_color <- cols[as.factor(spatial$epanutr_zoneid )]

wmap<-ggplot(data = spatial) + 
        geom_point(data = spatial, aes(x = lake_lon_decdeg, y = lake_lat_decdeg), color = spatial$ecoregion_color, size = 1) +  # Add points
        scale_fill_distiller( palette="Spectral", direction=1)+
        theme_void()+
       coord_fixed(1.3) +
        geom_polygon(data= states, aes(x = long, y = lat, group = group), fill="transparent",color = "black", size=0.5)

pdf(paste0(Sys.Date(),"_SPAITAL_sitesByEcoregion.pdf"),height=4, width=6)
           wmap
           dev.off()
           

           
           
           
#look at the initial dataframe used to make             
load("archive/2023-10-18_Limnosat_May_Oct_Lagos.RData")
objects()
spatial<-limnosat_may_oct_lagos
spatial<-spatial[,names(spatial)%in% c("epanutr_zoneid","lake_lon_decdeg","lake_lat_decdeg")]
spatial<-distinct(spatial)

# Get state borders
states <- map_data("state")

cols <- c(
  "dodgerblue2", "#E31A1C", # red
  "green4",
  "#6A3D9A", # purple
  "#FF7F00", # orange
   "gold1",
  "skyblue2", "#FB9A99", # lt pink
  "palegreen2",
  "#FDBF6F", # lt orange
  "maroon", "orchid1", "black","deeppink1", "blue1", "steelblue4",
  "darkturquoise", "green1", "yellow4", "yellow3",
  "darkorange4", "brown","gray","khaki2","#CAB2D6"
)

spatial$ecoregion_color <- cols[as.factor(spatial$epanutr_zoneid )]

wmap<-ggplot(data = spatial) + 
        geom_point(data = spatial, aes(x = lake_lon_decdeg, y = lake_lat_decdeg), color = spatial$ecoregion_color, size = 1) +  # Add points
        scale_fill_distiller( palette="Spectral", direction=1)+
        theme_void()+
       coord_fixed(1.3) +
        geom_polygon(data= states, aes(x = long, y = lat, group = group), fill="transparent",color = "black", size=0.5)

pdf(paste0(Sys.Date(),"_Limnosat_May_Oct_Lagos_sitesByEcoregion.pdf"),height=4, width=6)
           wmap
           dev.off()
```



















Question - what types of lakes were removed with the temporal dataset?

# 3. subset and average climate data

initial exploration
```{r}
climate<-readRDS("climate_data.RDS")
load("2023-10-19_SPATIAL_Limnosat_May_Oct_Lagos_Landcover.RData")

names(spatial)
names(climate)

table(unique(spatial$hu12_zoneid) %in%unique(climate$hu12_zoneid))
table(unique(climate$hu12_zoneid) %in%unique(spatial$hu12_zoneid))

#subset climate data by the hu12_zoneids we have in the spatial dataframe
nrow(climate)
48867504
climate<-climate[climate$hu12_zoneid %in%unique(spatial$hu12_zoneid),]
nrow(climate)
19236420/48867504
#[1] 0.3936444 #climate data shrunk when we only kept lakes with the hu12_zoneids we wanted, 40% of the total climate data

#How many unique hu12_zoneids?
length(unique(climate$hu12_zoneid))
#32715

#subset climate data by the years we have in limnosat

range(spatial$year)
#[1] 1984 2021
range(climate$climate_year)
#[1] 1970 2018

```

to test on my local drive, grab a subset of the climate data and download
```{r}
#on supercomputer
climate<-readRDS("climate_data.RDS")
climate_2<-rbind(climate[climate$hu12_zoneid==unique(climate$hu12_zoneid)[1],], climate[climate$hu12_zoneid==unique(climate$hu12_zoneid)[2],])
unique(climate_2$hu12_zoneid)
write.csv(climate_2,"/gscratch/jvonegge/LakeColor/TinySubsetClimateData.csv")

#in local computer
climate<-read.csv("/Users/jordanscheibe/Library/CloudStorage/OneDrive-UniversityofWyoming/Collaborations/LakeColor/TinySubsetClimateData.csv",header=T,row.names=1)

#subset climate data by the years we have in limnosat
climate<-climate[climate$climate_year %in% seq(1984,2021,1),]

```




2023-11-1_subset_average_ClimateData.R
```{r}
require(tidyverse)
#read in data files, using spatial because more complete sites (TEMPORAL is subset)
climate<-readRDS("../climate_data.RDS")
load("../2023-10-19_SPATIAL_Limnosat_May_Oct_Lagos_Landcover.RData")

#subset climate data by the hu12_zoneids we have in the spatial dataframe
climate<-climate[climate$hu12_zoneid %in% unique(spatial$hu12_zoneid),]

#subset climate data by the years we have in limnosat
climate<-climate[climate$climate_year %in% unique(spatial$year),]

#order dataframe so that the final file is in order of hu12_zoneid and years
climate<-climate %>%
        arrange(hu12_zoneid,climate_year,climate_month)

# average climate data by MAAT and cumulative precipitation, also record the total number of months of observations

#create empty data frame with final, averaged dataset and column names that will match the limnosat dataframe (climate_year --> year)
climate_YrAvg<-data.frame(hu12_zoneid=character(), year=numeric(), yr_tmean_degc=numeric(), yr_cumu_ppt_mm=numeric(), num_months=numeric(), perc_datacovperc_less100=numeric()) #

i=1
j=1
for(i in 1:length(unique(climate$hu12_zoneid))){
        sub<-climate[climate$hu12_zoneid==unique(climate$hu12_zoneid)[i],]
        for(j in 1:length(unique(sub$climate_year))){
                sub2<-sub[sub$climate_year==(unique(sub$climate_year))[j],]
                climate_YrAvg<-rbind(climate_YrAvg, data.frame(hu12_zoneid=sub2$hu12_zoneid[1], year=sub2$climate_year[1], yr_tmean_degc=mean(sub2$climate_tmean_degc), yr_cumu_ppt_mm=sum(sub2$climate_ppt_mm), num_months=length(unique(sub2$climate_month)), perc_datacovperc_less100=1-(length(which(sub2==100))/nrow(sub2))))
        }
        print(i)
}


climate<-climate_YrAvg #overwrite starting climate dataframe

rm(list=setdiff(ls(), "climate"))
save.image(paste0("../",Sys.Date(),"_ClimateData_Temp_Precip_YrAvg.Rdata"))
```


run_2023-11-1_subset_average_ClimateData.sh
```{bash}
#!/bin/bash
#SBATCH --job-name climate_avg
#SBATCH --mem=100GB
#SBATCH --time=4-00:00:00
#SBATCH --cpus-per-task=1
#SBATCH --account=microbiome
#SBATCH --output=climate_avg_%A.out

cd /project/lakecolor/data/archive
module load arcc/1.0  gcc/12.2.0 
module load r/4.2.2

srun Rscript 2023-11-1_subset_average_ClimateData.R
date
```



##### OLD BELOW







#Data exploration

```{r}
require(tidyverse)
load("new_limnosat_20230927.RData")
lake_data<-readRDS("20230929_lagos_data.rds")
load("2023-10-13_JordyDataExploration.RData")

#how many lakes in limnosat data?
length(unique(new_limnosat$lake_nhdid))
#192446

#how many lakes in limnosat have lagos data in general?
table(unique(new_limnosat$lake_nhdid)%in%unique(lake_data$lake_nhdid))

 # FALSE   TRUE 
 # 44920 147526 
 # 
 44920/147526 # approx 30% of the lakes don't have any Lagos data, 70% do (could still use lake area etc.)

#how many lakes in limnosat have Lagos data for the correct year?
limno_ids<-paste(new_limnosat$lake_ndhid,new_limnosat$year, sep="_")
lagos_ids<-paste(lake_data$lake_ndhid,lake_data$year, sep="_")
table(limno_ids%in%lagos_ids)
#    FALSE     TRUE 
# 50463867 13145726
13145726/50463867 #26% of the limnosat data has lagos data for the same year. 

#what is the lagos year range?
unique(lake_data$year)
# 2001 2004 2006 2008 2011 2013 2016

#what is the limnosat year range?
unique(new_limnosat$year)
# [1] 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998
# [16] 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013
# [31] 2014 2015 2016 2017 2018 2019 2020 2021

names(lake_data)
#  [1] "lagoslakeid"                 "lake_nhdid"                 
#  [3] "epanutr_zoneid"              "hu12_zoneid"                
#  [5] "lake_elevation_m"            "lake_lat_decdeg"            
#  [7] "lake_lon_decdeg"             "lake_waterarea_ha"          
#  [9] "lake_perimeter_m"            "lake_islandperimeter_m"     
# [11] "lake_connectivity_class"     "lake_connectivity_permanent"
# [13] "lake_totalperimeter_m"       "ws_area_ha"                 
# [15] "ws_lake_arearatio"           "year"                       
# [17] "datacoveragepct"             "nlcd_barren31_pct"          
# [19] "nlcd_cultcrop82_pct"         "nlcd_devhi24_pct"           
# [21] "nlcd_devlow22_pct"           "nlcd_devmed23_pct"          
# [23] "nlcd_devopen21_pct"          "nlcd_forcon42_pct"          
# [25] "nlcd_fordec41_pct"           "nlcd_formix43_pct"          
# [27] "nlcd_grass71_pct"            "nlcd_icesnow12_pct"         
# [29] "nlcd_openwater11_pct"        "nlcd_past81_pct"            
# [31] "nlcd_shrub52_pct"            "nlcd_wetemerg95_pct"        
# [33] "nlcd_wetwood90_pct"   


names(new_limnosat)
# [1] "date"             "lake_nhdid"       "year"             "dWL"             
# [5] "brightness"       "normalized_green" "Nir"  


```


```{r}

```



```{r}
load("new_limnosat_20230927.RData")
lake_data<-readRDS("20230929_lagos_data.rds")

limnosat_fulldataset<-left_join(new_limnosat, lake_data)
#Joining with `by = join_by(lake_nhdid, year)`



# Extract only the month and day
new_limnosat$date_m_d <- format(new_limnosat$date, format = "%m-%d")

# Filter the data frame to include only dates between May 1 and October 31

start_date <- format(as.Date("2023-05-01"), format = "%m-%d")
end_date <- format(as.Date("2023-10-31"), format = "%m-%d")
July_Sept_Limnosat <- new_limnosat[new_limnosat$date_m_d >= start_date & new_limnosat$date_m_d <= end_date, ]
start_date; end_date
nrow(July_Sept_Limnosat)
#18421502
nrow(new_limnosat)
#63609593

18421502/63609593 #0.2896026 - 29% of the data is July-Spet
length(unique(July_Sept_Limnosat$lake_nhdid))
#176485

176485/192446 # we retain about 91% of the lakes when we subset by July- Sept 15
192446-176485 # only lose about 15k lakes..


#get rid of year plus column



```


# Thoughts/questions
- some of these have names like - is this normal?

[99969] "14884883-9aac-47a9-963f-9c23e4085918"  
[99970] "149087458"                             
[99971] "150661278"                             
[99972] "155197250"                             
[99973] "155197287"                             
[99974] "157313322"                             
[99975] "167309159"                             
[99976] "59984368"                              
[99977] "59984378"                              
[99978] "83012293"                              
[99979] "83027739"                              
[99980] "83031775"                              
[99981] "{012BBD13-B750-4F27-908A-0681C0331F65}"
[99982] "{0F2A7C6C-3302-41F0-A059-9C199DDB9EF8}"

- Do we want to exclude any data that doesn't have lagos metadata? At this point we don't have any location or other data in the limnosat data.

-Hawaii and AK?
- We merged Lagos data with the same ndh id and year, but we likely could go back through and make lagos data for each lake and merge using that. 
        - lat/lon
        - lake area, perimeter, island perimeter
        - watershed area
        - watershed lake area ratio
        - connectivity class
        - land cover - this could change..
        
- Bella's paper had MAAT and it was significant, I think we should find a way to include this data. PRISM data
        
        

SCRAP CODE


```{R}
# limno_lakeid_year<-new_limnosat[,c(2,3)]
# > nrow(limno_lakeid_year)
# [1] 63609593
# > limno_lakeid_year<-unique(limno_lakeid_year)
# ^[^C
# > require(stringr)
# > limno_ids_uniq<-unique(limno_ids)
# > limno_ids_uniq_split<-str_split_fixed(limno_ids_uniq,pattern="_",n = 2)
# > head(limno_ids_uniq_split)
#      [,1]        [,2]  
# [1,] "138277543" "1984"
# [2,] "138518530" "1984"
# [3,] "143764566" "1984"
# [4,] "143765598" "1984"
# [5,] "143766002" "1984"
# [6,] "143771932" "1984"
# > limno_ids_uniq_split<-as.data.frame(str_split_fixed(limno_ids_uniq,pattern="_",n = 2))
# > head(limno_ids_uniq_split)
#          V1   V2
# 1 138277543 1984
# 2 138518530 1984
# 3 143764566 1984
# 4 143765598 1984
# 5 143766002 1984
# 6 143771932 1984
# > nrow(limno_ids_uniq_split)
# [1] 5792324
# > names(limno_ids_uniq_split)<-c("lake_nhdid","year")
# > ObsYearPerLake<-limno_ids_uniq_split %>% count(lake_nhdid)
# > head(ObsYearPerLake)
#                             lake_nhdid  n
# 1 0007A9A3-C0D7-4C62-8A7E-E99A7B2CC1A1 34
# 2 00148dc0-0f34-4fe6-bc96-3b18ee27f74c 37
# 3 00791451-15D3-43A8-A3FE-3B3E405B6196  6
# 4 0079468d-5e7d-4e46-b4d5-70a1038fc59e 28
# 5 008e581a-498a-4a01-b53d-cf58eef5304f 38
# 6 0091a8e5-48f4-4a0f-9eb5-4d4e565b0868 19
# > rm(new_limnosat)
# > rm(lake_data)
# > save.image(paste0(Sys.Date(),"_JordyDataExploration.RData"))
# > pdf(paste0(Sys.Date(),"_HistObsYearsPerLake.pdf"),height=6,width=7)
# > hist(ObsYearPerLake$n)
# > dev.off()
```



I think we want Ecoregion

"","date","lake_nhdid","year","dWL","brightness","normalized_green","Nir",
-what is "Nir"

- GPS points with "lake_nhdid"

"lagoslakeid","epanutr_zoneid","hu12_zoneid","lake_elevation_m","lake_lat_decdeg","lake_lon_decdeg","lake_waterarea_ha","lake_perimeter_m","lake_islandperimeter_m","lake_connectivity_class","lake_connectivity_permanent","lake_totalperimeter_m","ws_area_ha","ws_lake_arearatio","datacoveragepct","nlcd_barren31_pct","nlcd_cultcrop82_pct","nlcd_devhi24_pct","nlcd_devlow22_pct","nlcd_devmed23_pct","nlcd_devopen21_pct","nlcd_forcon42_pct","nlcd_fordec41_pct","nlcd_formix43_pct","nlcd_grass71_pct","nlcd_icesnow12_pct","nlcd_openwater11_pct","nlcd_past81_pct","nlcd_shrub52_pct","nlcd_wetemerg95_pct","nlcd_wetwood90_pct"


some points have this as their lake_nhdid:
{586F6665-D094-4ECC-9283-A0F8201FED73}"

There is a lot of data that don't have any other metadata, do we want to include these?

We need to double check what header names are the same in the dataset when they merge... 


"How can you find the unique values of the 10th column in a CSV file in linux?"
```{bash}
cut -d ',' -f 10 2023-10-03_FullDataset_Limnosat_LAGOS.csv | sort | uniq

```
output: 
"epanutr_1"
"epanutr_2"
"epanutr_3"
"epanutr_4"
"epanutr_5"
"epanutr_6"
"epanutr_7"
"epanutr_8"
"epanutr_9"
"epanutr_zoneid"
NA

cut -d ',' -f 10 2023-10-04_Limnosat_2018.csv | sort | uniq
#there are no ecoregions in the 2018 data.. so where are the ecoregions in terms of dates?

/Users/jordanscheibe/Desktop/2018_hist.R


looping in bash (can do $s, but if using an underscore after ${s}_)
troubleshooting a bash file (set -x, I believe)
